{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e728ea0b",
   "metadata": {},
   "source": [
    "# Practice Session 4: Hypothesis Testing I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a70f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import t, norm, ttest_1samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9bb5b",
   "metadata": {},
   "source": [
    "## Part 1: *t*-Distribution\n",
    "Yesterday, we talked about the normal distribution and saw that roughly 95% of the area under the normal curve\\\n",
    "lies within $\\mu \\pm 1.96\\sigma$ (that is, within approximately 1.96 standard deviations of the mean).\\\n",
    "We also saw that when we repeatedly drew samples and calculated their means, the histogram of those sample means looked bell-shaped.\\\n",
    "That was exactly the **Central Limit Theorem (CLT)** in action.\\\n",
    "Regardless of the original data’s distribution (as long as the observations are independent and identically distributed with finite variance):\n",
    "- the sampling distribution of the mean $\\bar{X}$ becomes approximately normal distribution as the sample size *n* grows,  \n",
    "- centered at the true mean $\\mu$,\n",
    "- with a spread described by the **standard error**:\\\n",
    " $\\text{SE} = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "So according to the CLT, the mean of samples follows approximately $\\bar{X} \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)$ \\\n",
    "This in turn means that $\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "Based on the properties of the standard normal distribution and after a few algebraic steps we can write:\n",
    "$P\\left(\\bar{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95$\\\n",
    "This result defines the **95% confidence interval** for the population mean $\\mu$.\n",
    "\n",
    "> It’s important to interpret this correctly:\n",
    "> The 95% confidence level means that *if we repeated the sampling process many times*,\n",
    "> about **95% of those intervals** would contain the true mean $\\mu$.\\\n",
    "> So the confidence level refers to the **method’s reliability**, not the probability that $\\mu$ lies inside one specific interval.\n",
    "\n",
    "\n",
    "Let’s now connect this theory to a concrete example.\n",
    "\n",
    "Yesterday, we considered a population that followed a normal distribution with a **mean of 10.0** and a **standard deviation of 1.8**, that is: $\\mathcal{N}(10.0, 1.8^2)$\n",
    "\n",
    "If we repeatedly draw random samples of size $n = 5$ from this population, the CLT tells us that the sample means will be approximately normally distributed as well:\\\n",
    "$\\bar{X} \\sim \\mathcal{N}\\left(10.0, \\frac{1.8^2}{5}\\right)$\\\n",
    "Based on this, we can use the formula derived above to construct a 95% confidence interval for the population mean:\\\n",
    "$\\bar{X} \\pm 1.96 \\times \\frac{1.8}{\\sqrt{5}}$\n",
    "\n",
    "So, if we were to repeat this sampling process many times, about **95% of the calculated intervals** would include the true mean value of **10.0**.\n",
    "\n",
    "However, in real-world situations we typically **do not know** the true standard deviation $\\sigma = 1.8$.\\\n",
    "Instead, we have to estimate it from the sample using the sample standard deviation ($s$).\n",
    "\n",
    "If we now use this sample estimate in our formula for the 95% confidence interval, we would write: $\\bar{X} \\pm 1.96 \\cdot \\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "Let’s check whether this version of the interval still captures the true mean about 95% of the time when the sample size is small ($n = 5$).\n",
    "\n",
    "Let’s test this empirically by simulation.\n",
    "\n",
    "We will:\n",
    "1. Draw many samples (100000) of size $n = 5$ from $\\mathcal{N}(10.0, 1.8^2)$.\n",
    "2. For each sample, compute $\\bar{X}$ and $s$.\n",
    "3. Build the interval $\\bar{X} \\pm 1.96 \\cdot \\frac{s}{\\sqrt{n}}$.\n",
    "4. Check whether $\\mu = 10.0$ falls inside.\n",
    "5. Calculate the **coverage proportion** across all repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e81b2",
   "metadata": {},
   "source": [
    "As we see, the coverage is lower - only about **88%**, not 95%.  So, why does this happen?\n",
    "\n",
    "In our simulation, the population standard deviation $\\sigma$ was **unknown**, so we estimated it using the sample standard deviation $s$.\\\n",
    "When the sample size $n$ is **large**, this substitution has very little effect - the sampling distribution of the statistic\n",
    "$\\frac{\\bar{X} - \\mu}{s / \\sqrt{n}}$ remains approximately **standard normal**.  \n",
    "This gives us the familiar large-sample confidence interval for the mean: $\\bar{X} \\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}$,\\\n",
    "which is valid for sufficiently large samples (usually $n \\ge 30\\text{–}40$), regardless of the exact shape of the population distribution  \n",
    "\n",
    "However, when the **sample size is small**, replacing $\\sigma$ with $s$  adds noticeable **extra variability**.  \n",
    "The statistic above is no longer normally distributed, and using the standard normal critical value $z_{\\alpha/2}=1.96$\n",
    "produces confidence intervals that are **too narrow** - as we just observed.\n",
    "\n",
    "To account for this additional uncertainty, we use a different reference distribution: the [**Student’s *t*-distribution**](https://onlinestatbook.com/2/estimation/t_distribution.html).\n",
    "\n",
    "#### The *t*-distribution\n",
    "\n",
    "The *t*-distribution was developed in [1908 by William Sealy Gosset](https://seismo.berkeley.edu/~kirchner/eps_120/Odds_n_ends/Students_original_paper.pdf), a statistician working at the Guinness Brewery in Dublin.  \n",
    "Because his employer required anonymity, he published under the pseudonym **“Student”**, hence the name **Student’s *t*-distribution**.\n",
    "\n",
    "The *t*-distribution applies when:\n",
    "- the underlying population is approximately normal,\n",
    "- the sample size $n$ is small,\n",
    "- and the population standard deviation $\\sigma$ is unknown and estimated by $s$.\n",
    "\n",
    "It is **symmetric** like the normal distribution, but has **heavier tails**, reflecting the extra uncertainty from estimating $\\sigma$.  \n",
    "The exact shape depends on the **degrees of freedom** ($\\text{df} = n - 1$).  \n",
    "As the sample size increases, the *t*-distribution gradually approaches the **standard normal**.\n",
    "\n",
    "Let’s visualize this next to see how the *t*-distribution compares to the normal distribution for different sample sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fac31b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2b0deeb9834814af0ccddfec0d4dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='df', min=1), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x values for the density plot\n",
    "x = np.linspace(-5, 5, 400)\n",
    "\n",
    "# Standard normal PDF (fixed)\n",
    "normal_pdf = norm.pdf(x)\n",
    "\n",
    "def plot_t_vs_normal(df=4):\n",
    "    \"\"\"Plot the t-distribution for a given df compared to the standard normal.\"\"\"\n",
    "    t_pdf = t.pdf(x, df)\n",
    "    \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(x, normal_pdf, 'k--', lw=2, label='Normal (Z)')\n",
    "    plt.plot(x, t_pdf, color='#00bf63', lw=2, label=f't-distribution (df={df})')\n",
    "  \n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.ylim(0, 0.45)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive slider for df\n",
    "interact(plot_t_vs_normal, df=IntSlider(min=1, max=100, step=1, value=4, description='df'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac47c5",
   "metadata": {},
   "source": [
    "> As we can see, both curves are symmetric and centered at zero, but the *t*-distribution has heavier tails.  \n",
    "> This means that extreme values are more likely - reflecting the extra uncertainty when estimating the standard deviation from a small sample.\n",
    ">\n",
    "> As the degrees of freedom increase (df = 10, 30, ...), the *t*-curve gradually approaches the standard normal.  \n",
    "> For large samples (roughly $n > 30$), the difference becomes negligible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e3110",
   "metadata": {},
   "source": [
    "#### <font color='#fc7202'>Task 1:</font>\n",
    "\n",
    "In the previous simulation, we saw that using the normal critical value (1.96) with a small sample size ($n = 5$) gave us coverage of only about **88%**, not 95%.  \n",
    "Now, let’s see if using the *t*-distribution instead fixes the problem.\n",
    "\n",
    "1. Repeat the same simulation as before - draw many samples of size $n = 5$ from  $\\mathcal{N}(10.0, 1.8^2)$.\n",
    "2. For each sample:\n",
    "   - Compute the sample mean ($\\bar{X}$) and the sample standard deviation ($s$).\n",
    "   - Construct a 95% confidence interval using the *t*-distribution:\\\n",
    "     $\\bar{X} \\pm t_{0.975,\\,df} \\times \\frac{s}{\\sqrt{n}}$,\n",
    "     where $df = n - 1$.\n",
    "3. Check in how many of these intervals the true mean $\\mu = 10.0$ falls.\n",
    "4. Compute the coverage proportion (should be close to 0.95).\n",
    "\n",
    "> *Hint:*\n",
    "> You can get the correct *t*-critical value ($t_{0.975, df}$) using `SciPy`:\n",
    "> ```python\n",
    "> df = n - 1\n",
    "> t_crit = t.ppf(0.975, df)   # 0.975 corresponds to the upper tail for a 95% CI\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e60bc",
   "metadata": {},
   "source": [
    "> You should now see a **coverage** very close to **0.95** (typically around 0.949 - 0.952 depending on random seed).\\\n",
    "> This confirms that using the *t*-distribution instead of the normal distribution correctly adjusts for the additional uncertainty from estimating $\\sigma$ with $s$.\\\n",
    "> As the sample size $n$ increases, the *t*-distribution approaches the normal distribution, and both methods yield almost identical results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbe4dc",
   "metadata": {},
   "source": [
    "*You might be wondering now, why are we spending so much time on t-distribution when today’s topic is **Hypothesis Testing**?*\\\n",
    "*The t-distribution forms the basis of the t-test, which we’ll start exploring next.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f3d23",
   "metadata": {},
   "source": [
    "## Part 2: Introduction to Statistical Inference and Hypothesis Testing\n",
    "\n",
    "By *inference*, we refer to a formal process of drawing conclusions from data.  \n",
    "The goal of statistical inference is to reach conclusions that are supported by evidence, not merely by observation or intuition.  \n",
    "In statistics, evidence arises through the careful and reasoned application of statistical methods and the evaluation of probabilities.\n",
    "\n",
    "To carry out a **hypothesis test**, we always begin with a **question** - what do we want to find out from our data?\n",
    "\n",
    "---\n",
    "\n",
    "Once the research question is clear, the **frequentist framework** guides us through  a formal and reproducible sequence of steps:\n",
    "\n",
    "1. **Formulate the hypotheses**  \n",
    "   We express the research question as two competing statements:  \n",
    "   - **Null hypothesis ($H_0$)** - assumes there is **no real effect or difference**, and any variation we observe is due to **random chance**.  \n",
    "   - **Alternative hypothesis ($H_1$)** - represents the claim that there **is** a real effect or that the parameter differs from the value stated in $H_0$.\n",
    "   \n",
    "   These two statements must cover **all possibilities** and be **mutually exclusive**.\\\n",
    "   It’s important to note that we can **never prove $H_0$ to be true** - we can only **fail to reject it** based on the available data.  \n",
    "   Statistical tests are designed to look for **sufficient evidence to reject $H_0$**, not to confirm it.\n",
    "\n",
    "2. **Establish the test statistic (and its null distribution)**  \n",
    "   Identify which **test statistic** will be used to compare $H_0$ and $H_1$.  \n",
    "   This choice depends on:\n",
    "   - the **type of data** (e.g., means, proportions, variances),  \n",
    "   - the **experimental design** (e.g., one-sample, independent two-sample, paired), and  \n",
    "   - what is **known about the population** (e.g., whether the population standard deviation $\\sigma$ is known).  \n",
    "\n",
    "   Under the null hypothesis, this statistic follows a **known reference (sampling) distribution**, such as the  *z*, *t*, *χ²*, or *F* distribution.\n",
    "\n",
    "3. **Set the decision rule (choose $\\alpha$ and define rejection criteria)**  \n",
    "   Next, define the rule for deciding when to reject $H_0$:  \n",
    "   - Choose the **significance level** $\\alpha$, which represents the probability of making a *Type I error* (rejecting a true $H_0$).  \n",
    "     The most common choice is **$\\alpha = 0.05$**, but stricter values (e.g., 0.01) may be used in sensitive applications.  \n",
    "   - Using the null distribution, determine the **critical value(s)** that define the **rejection region** -  \n",
    "     the range of test statistic values that would be considered unlikely if $H_0$ were true.  \n",
    "   - *Equivalent p-value approach:* reject $H_0$ if the computed ***p*-value** ≤ **α** (adjusted for one- or two-tailed tests).\n",
    "\n",
    "4. **Collect data, compute the test statistic, and compare**  \n",
    "   Gather the sample data and calculate the observed value of the chosen test statistic.  \n",
    "   Compare this value with the **critical region** (or use the *p*-value approach):  \n",
    "   - If the test statistic falls inside the rejection region or if *p* ≤ *α*, → **Reject $H_0$**.  \n",
    "   - Otherwise, → **Fail to reject $H_0$**.\n",
    "\n",
    "5. **Interpret the result in context**  \n",
    "   A statistical result only has meaning when connected back to the original question.  \n",
    "   - Clearly report whether $H_0$ was rejected or not, and **what that implies about the research question**.  \n",
    "   - Remember: “Failing to reject $H_0$” does *not* prove that $H_0$ is true - it simply means there was **not enough evidence** to conclude otherwise.  \n",
    "   - Whenever possible, complement the test result with **effect sizes** and **confidence intervals** to provide a fuller picture of the findings.\n",
    "---\n",
    "\n",
    "#### One-Tailed or Two-Tailed Tests\n",
    "Based on how we formulate our hypotheses, a test can be **one-tailed** or **two-tailed**.  \n",
    "- If the alternative hypothesis states only that the parameter is *different* from the null value ($H_1\\!:\\,\\mu \\ne \\mu_0$), the test is **two-tailed**,\\\n",
    "and the significance level $\\alpha$ is divided equally between both tails of the distribution.  \n",
    "- If the alternative specifies a *direction* of difference ($H_1\\!:\\,\\mu > \\mu_0$ or $H_1\\!:\\,\\mu < \\mu_0$), the test is **one-tailed**, and the entire $\\alpha$ lies in a single tail.  \n",
    "\n",
    "This choice must be made **before** the analysis and should always reflect the research question.\n",
    "\n",
    "\n",
    "#### Type I and Type II Errors\n",
    "\n",
    "When we make decisions based on sample data, there are **two kinds of mistakes** we can make.\n",
    "These are called **Type I** and **Type II** errors.\n",
    "\n",
    "Whenever we perform a hypothesis test, there are two possible *truths* (whether $H_0$ is true or false) and two possible *decisions* (whether we reject $H_0$ or not).  \n",
    "The four combinations lead to the following outcomes:\n",
    "\n",
    "| **Null hypothesis ($H_0$) is ...**| **True** | **False** |\n",
    "|-------------------------|----------------|-----------------|\n",
    "| **Rejected** | ❌ **Type I error** (false positive) | ✅ **Correct decision** (true positive) |\n",
    "| **Failed to reject** | ✅ **Correct decision** (true negative) | ❌ **Type II error** (false negative) |\n",
    "\n",
    "- **Type I error (α):**  \n",
    "  Occurs when we *reject a true null hypothesis*.  \n",
    "  In other words, we detect an effect that **does not actually exist**.  \n",
    "  The probability of making this error is the **significance level**,  \n",
    "  typically $\\alpha = 0.05$ (5%).\n",
    "\n",
    "- **Type II error (β):**  \n",
    "  Occurs when we *fail to reject a false null hypothesis*.  \n",
    "  In other words, we **miss a real effect** that does exist.  \n",
    "  The probability of this mistake is denoted by **β**.\n",
    "\n",
    "- **Power of a test (1 − β):**  \n",
    "  The probability of correctly rejecting a false $H_0$.  \n",
    "  A more *powerful* test has a smaller chance of missing true effects.\n",
    "\n",
    "**How to interpret α and β**\n",
    "- The smaller the **α**, the less likely we are to make a **Type I error**, but the harder it becomes to detect true effects (increasing **β**).  \n",
    "- Conversely, increasing **α** makes it easier to find effects (reducing **β**), but increases the risk of false positives.  \n",
    "- Therefore, α and β are **interconnected** - lowering one usually raises the other.\n",
    "\n",
    "In practice, we choose α (e.g., 0.05) before testing, and aim to design studies with high **power** (often ≥ 0.8), to minimize the risk of both errors as much as possible.\n",
    "\n",
    "![errors](https://www.researchgate.net/publication/361295532/figure/fig2/AS:11431281100133326@1669325756681/The-relationship-between-a-type-I-error-alpha-and-a-type-II-error-beta-Note.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296f7a3",
   "metadata": {},
   "source": [
    "> But the main question in practice is often: **how do we choose the right statistical test?**  \n",
    "> Thanks to Matt, we have a helpful guideline image that summarizes this decision process.  \n",
    "> Today, we’ll focus on one of the most common tests from that chart - the ***t*-test**.\n",
    "![what_test](https://raw.githubusercontent.com/RaHub4AI/MI7032/refs/heads/main/Pictures/Hypothesis%20Testing%20Key%202023.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025b60a",
   "metadata": {},
   "source": [
    "#### Putting Theory into Practice\n",
    "\n",
    "Let’s now put all this theory into practice.  \n",
    "We’ll start with one of the most widely used tools in data analysis - the ***t*-test**.\n",
    "\n",
    "The *t*-test allows us to compare **means** and decide whether any observed difference is **statistically significant** or simply due to random variation.\n",
    "\n",
    "Depending on the question we want to answer, there are three main types of *t*-tests:\n",
    "\n",
    "1. **One-sample *t*-test**  \n",
    "   Used to compare the **mean of a single sample** against a **known or reference value**.  \n",
    "   Statistic: $t = \\frac{\\bar{X} - \\mu_0}{s / \\sqrt{n}}$, where:  \n",
    "      - $\\bar{X}$ = sample mean  \n",
    "      - $\\mu_0$ = hypothesized (reference) mean  \n",
    "      - $s$ = sample standard deviation  \n",
    "      - $n$ = sample size  \n",
    "\n",
    "      *Example:* Testing whether the mean concentration measured in a reference material differs from the certified value.\n",
    "\n",
    "2. **Independent two-sample *t*-test**\n",
    "   Used to compare the **means of two independent groups**  (e.g., two different treatments or populations).\\\n",
    "   Statistic: $t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$, where:\n",
    "   - $s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}$ is the **pooled standard deviation**, and\n",
    "   - $n_1$, $n_2$, $s_1$, $s_2$ are the sample sizes and standard deviations of the two groups.\n",
    "\n",
    "   *Example:* Comparing the average pollutant concentration in two different rivers.\n",
    "\n",
    "3. **Paired-sample (dependent) *t*-test**\\\n",
    "   Used when the two samples are **related**, for example, measurements taken **before and after** a treatment on the same individuals.\\\n",
    "   Statistic: $t = \\frac{\\bar{D}}{s_D / \\sqrt{n}}$, where:  \n",
    "      - $\\bar{D}$ = mean of the differences between paired observations  \n",
    "      - $s_D$ = standard deviation of the differences  \n",
    "      - $n$ = number of pairs  \n",
    "\n",
    "   *Example:* Temperature measurements made at the same time, but different locations\n",
    "\n",
    "\n",
    "In the next step, we’ll begin with the simplest case - the **one-sample *t*-test** - to test whether our measured mean differs significantly from a known reference value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8f109",
   "metadata": {},
   "source": [
    "#### <font color='#fc7202'>Task 2: </font>\n",
    "You purchase a **certified reference material (CRM)** for *lead (Pb)* concentration in rainwater. According to **NIST**, the certified concentration is **190 ng/L**.\\\n",
    "You analyze this reference sample **ten times** using your instrument and obtain the following results (in ng/L):\n",
    "$187,\\; 171,\\; 191,\\; 176,\\; 196,\\; 181,\\; 189,\\; 190,\\; 185,\\; 189$\n",
    "\n",
    "1. Test whether your measurements **differ** from the certified value at the **95% confidence level**.\n",
    "2. Test whether your measurements are **significantly lower** than the certified value  \n",
    "   at the **95% confidence level**.\n",
    "\n",
    "> *Hints:*\n",
    "> Start by clearly stating your hypotheses ($H_0$ and $H_1$).   \n",
    "> Check the assumptions of the one-sample *t*-test:  \n",
    ">    - The data are independent.  \n",
    ">    - The measurements come from an approximately normal distribution (you can visualize or test this).\n",
    ">\n",
    "> To perform the test, you can use the built-in function `scipy.stats.ttest_1samp()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a16fc",
   "metadata": {},
   "source": [
    "<font color='#00bf63'>*Your hypotheses here!*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d25386",
   "metadata": {},
   "source": [
    "<font color='#00bf63'>*Your answer here!*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae6927",
   "metadata": {},
   "source": [
    "#### <font color='#fc7202'>Task 3:</font>\n",
    "\n",
    "You are developing a new extraction and analysis method for **carbon tetrachloride** in air.  \n",
    "In spike/recovery experiments, you add **exactly 50 ng** of carbon tetrachloride to air in a closed chamber.  \n",
    "Five experiments yield the following measurements (ng): $50.4,\\; 50.7,\\; 49.1,\\; 49.0,\\; 51.1$\n",
    "\n",
    "Is there evidence of **systematic error (bias)** in your analysis at the **95% confidence level**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff518d",
   "metadata": {},
   "source": [
    "<font color='#00bf63'>*Your hypotheses here!*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1adf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89b0f3",
   "metadata": {},
   "source": [
    "<font color='#00bf63'>*Your answer here!*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038590e8",
   "metadata": {},
   "source": [
    "So far, we have compared a **sample mean** to a **known reference value** using the one-sample *t*-test.  \n",
    "Next, we’ll see how to compare the **means of two groups** - using the **two-sample *t*-test**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb6b92",
   "metadata": {},
   "source": [
    "#### <font color=\"#fc7202\">Task 4:</font>\n",
    "\n",
    "Two methods for determining **chromium** concentrations in grass were applied to the **same samples**.  \n",
    "Results (units consistent):\n",
    "\n",
    "| Sample | Method 1 | Method 2 |\n",
    "|:-----:|:--------:|:--------:|\n",
    "| 1 | 1.79 | 2.01 |\n",
    "| 2 | 1.74 | 2.81 |\n",
    "| 3 | 1.41 | 2.34 |\n",
    "| 4 | 1.29 | 2.12 |\n",
    "| 5 | 1.15 | 2.39 |\n",
    "\n",
    "Do the two methods give results with **means that differ significantly** at the 95% confidence level?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0814c4",
   "metadata": {},
   "source": [
    "<font color='#00bf63'>*Your hypotheses here!*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc38b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794321",
   "metadata": {},
   "source": [
    "<font color='#00bf63'>*Your answer here!*</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
