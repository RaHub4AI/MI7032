# Environmental Data Science Glossary

This glossary collects key terms and definitions that will be useful throughout this course.
It is designed as a quick reference to help you understand core concepts in **programming** and **statistics**.

---

<details>

<summary>A</summary>

#### Accuracy
Accuracy is a measure of how often a predictive model correctly predicts the outcome. In a two-class problem, it is defined as the ratio of the number of times a machine learning model correctly recognizes events of the two classes with respect to all events in the dataset.

#### Alternative hypothesis
When trying to understand the effect of an independent variable on a dependent variable, the alternative hypothesis is the claim that there is such an effect. Researchers use a statistical test to weigh evidence for or against the alternative hypothesis.

#### ANOVA
Analysis of variance (ANOVA) is a statistical method used to determine if there are statistically significant differences between the means of three or more independent groups by analyzing the variation within each group compared to the variation between the groups.

#### API
An application programming interface (API) is a software intermediary that allows two software applications to communicate with each other, enabling data exchange.

</details>


<details>

<summary>B</summary>

#### Bar chart
A bar chart is a graphical representation of categorical data using rectangular bars, where the length of each bar corresponds to the value it represents. It is commonly used for displaying category frequency, comparing different categories, or tracking changes in values over time.

#### Bayesian statistics
Bayesian statistics is a branch of statistics that uses probability to represent uncertainty in statistical models and updates these probabilities as new data becomes available. It incorporates prior knowledge or beliefs to refine predictions and decision-making.

#### Bayes’ theorem
Bayes’ theorem is a fundamental theorem in probability theory that quantifies the probability of an event based on prior knowledge of conditions related to the event.

#### Bernoulli trial
Bernoulli's trial is an experiment with either a success or a failure as the outcome. The probability of success is constant, and the trials are statistically independent—the outcome of one trial does not affect the outcome of another.

#### Bias
Bias is the systematic error in a model that affects its predictions by consistently skewing results in one direction, regardless of the training data size.

#### Big data
Big data refers to extremely large datasets that can be analyzed computationally to reveal patterns, trends, and associations—especially in relation to human behavior and interactions.

#### Binary classification
Binary classification is a type of predictive modeling that categorizes data into two distinct classes.

#### Binary variable
A binary variable is one that has only two possible values, such as true/false or yes/no.

#### Binomial distribution
The binomial distribution calculates the probability of a certain number of "successes" in a set number of independent tries (Bernoulli trials), each with the same probability of success.

#### Boolean
Boolean is a data type with two possible values: true or false.

#### Boosting
Boosting is an ensemble learning technique that improves prediction by combining several weak learners. Each model is trained to focus on the errors of its predecessor, and by weighting and combining these models, boosting reduces bias and increases stability.

#### Bootstrapping
Bootstrapping is a statistical sampling method where samples are drawn from the original dataset with replacement. This method helps estimate the sampling distribution of a statistic and approximate measures like variance and confidence intervals.

#### Box plot
A box plot is a graphical tool to visualize the distribution of a numeric variable. It shows the median, quartiles, and potential outliers, making it useful for comparing distributions.

</details>


<details>

<summary>C</summary>

#### Categorical variable
A categorical variable can take on a limited set of distinct values, each representing a different group or category. These values are mutually exclusive and collectively exhaustive.

#### Chi-square test
The chi-square test is a statistical method used to determine whether there is a significant association between observed and expected frequencies in categorical data, commonly applied to test model goodness-of-fit or variable independence.

#### Classification
Classification is a set of supervised learning techniques in which a model predicts the correct label for a given input based on its features.

#### Cluster analysis
Cluster analysis is an unsupervised learning technique used to group similar data points into clusters based on measures like distance or frequency, helping to reveal data structure and patterns.

#### Concatenate
For data tables: Concatenate combines two or more data tables by stacking them vertically. For strings: It creates a new string by joining two or more strings end-to-end.

#### Confidence interval
A confidence interval is a range derived from sample data that estimates where a population parameter likely falls, accompanied by a confidence level (e.g., 95%).

#### Confusion matrix
A confusion matrix is a table that displays the actual versus predicted classifications, showing true positives, true negatives, false positives, and false negatives to evaluate a classification model's performance.

#### Continuous probability distribution
A continuous probability distribution defines the likelihood of a continuous random variable taking on any value within a range, with probabilities represented by a density function that integrates to 1.

#### Continuous random variable
A continuous random variable can take an infinite number of values within a given range and is typically measured rather than counted.

#### Correlation
Correlation is a statistical measure describing the strength and direction of the relationship between two variables. Its values range from -1 (perfect negative) to 1 (perfect positive), with 0 indicating no linear correlation.

#### Cost function
In machine learning, a cost (or loss) function calculates the error between predicted values and actual values. It is used to guide the optimization of model parameters.

#### Covariance
Covariance measures how two variables change together. A positive covariance indicates that the variables tend to increase or decrease together, while a negative value indicates they move inversely.

#### Cross-validation
Cross-validation is a model evaluation technique that involves partitioning the data into subsets, training the model on some parts, and validating it on the remaining parts to assess its generalizability.

</details>


<details>

<summary>D</summary>

#### Data cleaning
Data cleaning involves identifying and correcting or removing errors, inconsistencies, or irrelevant information from datasets to improve data quality.

#### Data mining
Data mining is the process of discovering patterns and knowledge from large amounts of data by combining methods from statistics, machine learning, and database systems.

#### Data modeling
Data modeling is the process of creating a logical representation of business requirements and data relationships. Data science involves creating mathematical representations of real-world data to understand its structure and behavior.

#### Data pipeline
A data pipeline is a sequence of steps that moves and transforms data from its source to its destination, often including ingestion, processing, integration, and storage.

#### Data preparation
Data preparation involves transforming raw data into a format suitable for analysis or modeling, including tasks such as cleaning, formatting, and feature engineering.

#### Data science
Data science is an interdisciplinary field that combines data processing, machine learning, and statistics to extract knowledge and insights from both structured and unstructured data.

#### Data transformation
Data transformation is the process of converting data from one format, structure, or representation to another to meet specific requirements.

#### Data type
A data type defines the characteristics of a value, such as its numerical precision or storage format, categorizing values as numbers, text, dates, etc.

#### Data visualization
Data visualization is the graphical representation of data through charts, graphs, maps, or other visual elements to facilitate exploration, analysis, and communication.

#### Data wrangling
Data wrangling is the process of cleaning, restructuring, and enriching raw data into a desired format for better decision-making in analysis or modeling.

#### Database
A database is a structured collection of data stored electronically and organized for efficient retrieval, updating, and management.

#### Dataframe
A dataframe is a tabular data structure with rows and columns, similar to a spreadsheet or SQL table, commonly used in data manipulation and analysis.

#### Dataset
A dataset is a collection of related data organized for analysis, modeling, or training machine learning algorithms.

#### Decile
A decile divides a dataset into ten equal parts. Each decile represents 10% of the data, helping to understand the distribution of the dataset.

#### Decision boundary
A decision boundary is the dividing line (or hyperplane) that separates different classes in a dataset as determined by a classification algorithm.

#### Decision tree
A decision tree is a supervised learning algorithm that splits data into subsets based on decision rules, forming a tree-like model to predict outcomes.

#### Deep learning
Deep learning is a subset of machine learning that uses neural networks with many layers to model complex patterns in data, enabling advances in areas such as image and speech recognition.

#### Degree of freedom
Degree of freedom refers to the number of independent values or parameters that can vary in the calculation of a statistic.

#### Dependent variable
A dependent variable is the outcome or response that is measured in an experiment, influenced by changes in the independent variable(s).

#### Descriptive statistics
Descriptive statistics summarize and describe the main features of a dataset, using measures such as the mean, median, mode, and standard deviation.

#### Dimensionality reduction
Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables while preserving as much information as possible.

#### Discrete distribution
A discrete distribution is a probability distribution for a random variable that can take on a finite or countable number of values, with probabilities assigned to each value summing to 1.

#### Discrete random variable
A discrete random variable takes on distinct, separate values, such as integers, rather than a continuous range.

#### Dummy variable
A dummy variable is a binary variable (coded 0 or 1) used to represent the presence or absence of a categorical effect in modeling.

</details>


<details>

<summary>E</summary>

#### Ensemble learning
Ensemble learning combines the predictions of multiple models to produce a more robust and accurate overall prediction than any single model.

#### Evaluation metrics
Evaluation metrics are quantitative measures (such as accuracy, precision, recall, and F-score) used to assess the performance of a predictive model.

</details>


<details>

<summary>F</summary>

#### Factor analysis
Factor analysis is a statistical method used to describe variability among observed variables in terms of a smaller number of unobserved variables called factors. It helps reveal the underlying structure of data.

#### False negative
A false negative occurs in binary classification when a positive instance is incorrectly predicted as negative.

#### False positive
A false positive occurs in binary classification when a negative instance is incorrectly predicted as positive.

#### Feature engineering
Feature engineering is the process of using domain knowledge to create new features from raw data, thereby improving the performance of machine learning models.

#### Float
A float is a data type used to represent real numbers with a fractional component.

#### F-score
F-score is an evaluation metric that combines precision and recall into a single measure, often used to assess classification model performance.

#### *F*-test
An *F*-test is used to test if the variances of two populations are equal. This test can be a two-tailed test or a one-tailed test. The two-tailed version tests against the alternative that the variances are not equal. The one-tailed version only tests in one direction, that is the variance from the first population is either greater than or less than (but not both) the second population variance. The choice is determined by the problem. For example, if we are testing a new process, we may only be interested in knowing if the new process is less variable than the old process.

</details>


<details>

<summary>G</summary>

#### Gaussian distribution
The Gaussian distribution, or normal distribution, is a symmetric, bell-shaped probability distribution centered around the mean, with spread determined by the standard deviation.

#### Goodness-of-fit
Goodness-of-fit tests assess how well a statistical model or distribution matches the observed data.

#### Greedy algorithms
A greedy algorithm makes the locally optimal choice at each step with the hope of finding a global optimum, though it does not always guarantee the best overall solution.

</details>


<details>

<summary>H</summary>

#### Hidden Markov model
A hidden Markov model is a statistical model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states, used to infer the sequence of states from observable events.

#### Hierarchical clustering
Hierarchical clustering is a technique that builds a tree (dendrogram) of clusters by either progressively merging smaller clusters or splitting larger ones.

#### Histogram
A histogram is a graphical representation of the frequency distribution of a numerical variable, using bars to show the number of data points within specified ranges (bins).

#### Holdout sample
A holdout sample is a subset of data set aside during model training to evaluate its performance on unseen data.

#### Hyperparameter
A hyperparameter is a configuration set by the user (external to the model) that influences the training process and performance of a machine learning algorithm.

#### Hyperparameter tuning
Hyperparameter tuning is the process of selecting the optimal values for a model’s hyperparameters to achieve the best performance.

#### Hypothesis
A hypothesis is a proposed explanation or assumption made on the basis of limited evidence, which can then be tested using statistical methods.

</details>


<details>

<summary>I</summary>

#### IDE
An IDE (Integrated Development Environment) is software that combines commonly used developer tools into a compact GUI (graphical user interface) application. It is a combination of tools like a code editor, code compiler, and code debugger with an integrated terminal.

#### Imputation
Missing Value Imputation is the process of replacing missing data with substituted values to improve dataset completeness for analysis and modeling.

#### Independent variable
An independent variable is one that is manipulated or categorized to observe its effect on a dependent variable, often serving as a predictor in experiments and models.

#### Inferential statistics
Inferential statistics involves making predictions or inferences about a population based on a sample of data, using methods like hypothesis testing and confidence intervals.

#### Integer
An integer is a whole number (positive, negative, or zero) used in data analysis for counting and indexing.

#### Interquartile range
An interquartile range (IQR) is a measure of statistical dispersion calculated as the difference between the third quartile (Q3) and the first quartile (Q1), representing the middle 50% of data.

#### Iteration
Iteration refers to the repetition of a set of operations in algorithms or model training to gradually improve performance or converge on a solution. 

</details>


<details>

<summary>J</summary>

#### Joint probability
Joint probability is the probability of two or more events occurring simultaneously. If the events are independent, their joint probability is the product of their individual probabilities.

</details>


<details>

<summary>K</summary>

#### Kernel
A kernel is the essential foundation of a computer's operating system (OS). It's the core that provides basic services for all other parts of the OS. It's the main layer between the OS and underlying computer hardware, and it helps with tasks such as process and memory management, inter-process communication, file system management, device control and networking.

#### *K*-means
*K*-means clustering partitions data into *K* distinct clusters by iteratively updating cluster centroids until convergence.

#### *K*-nearest neighbors
*K*-nearest neighbors (KNN) is a supervised algorithm that predicts the class or value of a sample based on the classes or values of its *K* closest neighbors.

#### Kurtosis
Kurtosis measures the "tailedness" of a probability distribution, with high kurtosis indicating heavy tails and low kurtosis indicating light tails compared to a normal distribution.

</details>


<details>

<summary>L</summary>

#### Labeled data
Labeled data are records that have been tagged with target labels, making them essential for supervised learning.

#### Line chart
A line chart is a data visualization that displays information as a series of data points connected by straight lines, ideal for showing trends over time.

#### Linear regression
Linear regression is a supervised learning algorithm that predicts a continuous outcome based on one or more predictor variables by fitting a linear relationship to the data.

#### Log likelihood
Log likelihood is the natural logarithm of the likelihood function and is used in statistical modeling to estimate model parameters that best explain the observed data.

#### Log loss
Log loss, also known as logistic loss or cross-entropy, quantifies the error between predicted probabilities and actual outcomes in binary classification.

#### Logistic regression
Logistic regression is a supervised learning algorithm used for binary classification, predicting the probability of an outcome that can take one of two possible values.

#### Loops
Loops refer to the repeated execution of a block of code or workflow segment, continuing as long as a specified condition remains true.

</details>


<details>

<summary>M</summary>

#### Machine learning
Machine learning is a subset of artificial intelligence focusing on systems that can learn patterns and trends from data without being explicitly programmed. It is often used for making predictions and decisions.

#### Mean
The mean is the arithmetic average of a set of numbers, calculated by summing all the values and dividing by the count. It is a measure of central tendency in data.

#### Mean absolute error
Mean absolute error (MAE) is a measure of error in paired observations. It calculates the average absolute differences between two sequences of values. MAE is often used to evaluate numeric prediction models by comparing predicted values to actual values.

#### Mean squared error
Mean squared error (MSE) calculates the average squared differences between two sequences of values and is used to evaluate numeric prediction models.

#### Median
The median is the middle value in a sequence of ordered values. It divides the dataset into two halves, providing a robust measure of central tendency.

#### Mode
The mode is the value that appears most frequently in a dataset and is especially useful for categorical data.

#### Model selection
Model selection is the process of selecting the most appropriate model from a set of candidate models for a given dataset, often using cross-validation or other evaluation criteria.

#### Monte Carlo simulation
Monte Carlo simulation is a computational technique that uses random sampling to obtain numerical results, modeling the probability of different outcomes in complex systems.

#### Multi-class classification
Multi-class classification is a type of classification task where the goal is to assign group labels from three or more classes, as opposed to binary classification.

#### Multivariate analysis
Multivariate analysis examines multiple variables simultaneously to understand relationships, interactions, and effects on outcomes. It includes methods like multivariate regression and MANOVA.

#### Multivariate regression
Multivariate regression is an extension of linear regression that models the relationship between multiple independent variables and multiple dependent variables.

</details>


<details>

<summary>N</summary>

#### NaN
NaN stands for "Not a Number" and represents undefined or unrepresentable numerical results, often used to denote missing values.

#### Naive Bayes
Naive Bayes is a probabilistic classifier based on Bayes' theorem, assuming independence between predictors.

#### Nominal variable
A nominal variable is a categorical variable with distinct categories that have no inherent order, such as gender or color.

#### Normal distribution
See Gaussian distribution.

#### Normalization
Normalization is the process of scaling data to fall within a standard range—often between 0 and 1—or to have a standard distribution, as required by some machine learning algorithms.

#### Null hypothesis
The null hypothesis is the claim that there is no effect or relationship between variables, and it is tested against an alternative hypothesis.

#### Numeric prediction
Numeric prediction refers to predicting a continuous numerical value based on input data.

</details>


<details>

<summary>O</summary>

#### Ordinal variable
An ordinal variable is a categorical variable with a clear ordering (e.g., education level or satisfaction rating), though the differences between levels may not be uniform.

#### Outlier
An outlier is a data point that deviates significantly from the majority of observations, potentially indicating measurement error or a unique phenomenon.

#### Overfitting
Overfitting occurs when a model learns the noise in the training data instead of the underlying pattern, leading to poor generalization on unseen data.

</details>


<details>

<summary>P</summary>

#### Pearson correlation coefficient
The Pearson correlation coefficient measures the linear relationship between two variables, ranging from -1 (perfect negative) to 1 (perfect positive).

#### Pie chart
A pie chart is a circular graph divided into slices to illustrate numerical proportions, with each slice representing a category’s share of the whole.

#### Poisson distribution
A Poisson distribution is a discrete probability distribution that gives the probability of a given number of events occurring in a fixed interval, based on a known average rate and independence of events.

#### Polynomial regression
Polynomial regression is a supervised learning algorithm that models the relationship between a dependent variable and one or more independent variables as an nth degree polynomial.

#### Precision
Precision is the ratio of true positive predictions to the total number of positive predictions made by a classification model.

#### Predictive model
A predictive model uses statistical and machine learning techniques to learn patterns from historical data and make predictions about future outcomes.

#### Predictor variable
See independent variable.

#### Principal component analysis
Principal component analysis (PCA) is a dimensionality reduction technique that transforms data into a new coordinate system where the greatest variance lies on the first coordinate, the second greatest on the second, and so on.

#### Probability distribution
A probability distribution describes all possible values of a random variable along with their associated probabilities. It can be continuous or discrete.

#### Program
A program is a finite set of instructions written in a programming language that a computer executes to perform a specific task.

#### Programming language
A programming language is a formal system of instructions used to create software, with examples including Python, Java, and C++.

#### *p*-value
The *p*-value is the probability of obtaining a result at least as extreme as the observed one, assuming the null hypothesis is true. A low *p*-value suggests the result is unlikely under the null hypothesis.

</details>


<details>

<summary>Q</summary>

#### Q-Q plot
A Q-Q plot (quantile-quantile plot) is a graphical tool that compares the quantiles of two probability distributions to assess if they follow a common distribution.

#### Quartile
A quartile divides a ranked dataset into four equal parts. Q1, Q2 (the median), and Q3 represent the 25th, 50th, and 75th percentiles, respectively.

</details>


<details>

<summary>R</summary>

#### ROC curve
The ROC (Receiver Operating Characteristic) curve is a graph showing the performance of a classification model by plotting the true positive rate against the false positive rate at various thresholds.

#### ROC-AUC
ROC-AUC stands for Receiver Operating Characteristic – Area Under the Curve, measuring a classification model's ability to distinguish between classes; 1 indicates perfect distinction, while 0.5 suggests random guessing.

#### Random forest
Random forest is an ensemble learning method that builds multiple decision trees and combines their outputs to improve classification or regression accuracy.

#### Random sample
A random sample is a subset chosen from a population in such a way that every individual has an equal chance of being selected, ensuring representativeness.

#### Random variable
A random variable represents the possible outcomes of a random event; it can be discrete or continuous.

#### Range
Range is a measure of dispersion calculated as the difference between the maximum and minimum values in a dataset.

#### Recall
Recall (or sensitivity) is the ratio of true positives to the total actual positives, indicating a model’s ability to identify positive instances.

#### Regression
Regression is a statistical technique for modeling the relationship between a dependent variable and one or more independent variables.

#### Resampling
Resampling involves repeatedly drawing samples from a dataset to assess the variability of a statistic; techniques include bootstrapping and cross-validation.

#### Residuals
Residuals are the differences between observed values and the values predicted by a model, used to evaluate model fit.

#### Root mean squared error
Root Mean Squared Error (RMSE) is the square root of the average squared differences between predicted and actual values, used to assess the accuracy of numeric prediction models.

</details>


<details>

<summary>S</summary>

#### Sample
A sample is a subset of individuals or observations selected from a larger population for analysis.

#### Sampling error
Sampling error is the difference between a sample statistic and the corresponding population parameter, arising from the fact that only a subset of data is observed.

#### Scatter plot
A scatter plot is a graphical representation of the relationship between two variables, using Cartesian coordinates to display individual data points.

#### Selection Bias
Selection bias occurs when the data collection method results in a sample that is not representative of the population, leading to skewed or invalid conclusions.

#### Semi-supervised learning
Semi-supervised learning uses both labeled and unlabeled data for training, which is useful when acquiring labeled data is expensive or time-consuming.

#### Skewness
Skewness is a measure of the asymmetry of a probability distribution; positive skew indicates a longer right tail, while negative skew indicates a longer left tail.

#### Spatial-temporal reasoning
Spatial-temporal reasoning involves analyzing data that varies across both space and time, integrating concepts from computer science, cognitive science, and psychology to forecast or understand dynamic systems.

#### Spearman rank correlation
Spearman rank correlation is a non-parametric measure of rank correlation that assesses the strength and direction of the association between two ranked variables.

#### Standard deviation
Standard deviation quantifies the amount of variation or dispersion in a dataset relative to its mean.

#### Standard error
Standard error is the standard deviation of the sampling distribution of a statistic, typically used to measure the precision of the sample mean.

#### Standardization
Standardization scales data to have a mean of zero and a standard deviation of one, ensuring that all features contribute equally to model performance.

#### Statistics
Statistics is the science of collecting, analyzing, interpreting, and presenting data, used to make inferences about populations based on samples.

#### Stratified sampling
Stratified sampling divides the population into distinct subgroups (strata) and takes a random sample from each to ensure representativeness.

#### String
A string is a sequence of characters used to represent text in programming.

#### Structured data
Structured data is organized in a predefined format (typically rows and columns), as found in relational databases and spreadsheets.

#### Summary statistics
Summary statistics are concise measures (like mean, median, and standard deviation) that describe the main features of a dataset.

#### Supervised learning
Supervised learning is a machine learning paradigm where models are trained on labeled data to learn the relationship between inputs and outputs.

#### Support vector machine
Support vector machine (SVM) is a supervised algorithm that finds a hyperplane to separate classes with the widest possible margin, used in both classification and regression.

#### Synthetic data
Synthetic data is artificially generated to mimic the statistical properties of real-world data, used when actual data is scarce or sensitive.

</details>


<details>

<summary>T</summary>

#### *t*-test
A *t*-test is a statistical test used to determine whether there is a significant difference between the means of two groups.

#### Time series analysis
Time series analysis studies data points collected over time to identify patterns and trends, and to forecast future values.

#### Training and testing
Training and testing are phases in the machine learning workflow where a model is first trained on a dataset and then evaluated on unseen data to assess its performance.

#### True negative
A true negative is a correct prediction in binary classification where a negative instance is correctly identified as negative.

#### True positive
A true positive is a correct prediction in binary classification where a positive instance is correctly identified as positive.

#### Type I error
A Type I error occurs when a true null hypothesis is incorrectly rejected, commonly known as a false positive.

#### Type II error
A Type II error occurs when a false null hypothesis is not rejected, often referred to as a false negative.

</details>


<details>

<summary>U</summary>

#### Underfitting
Underfitting occurs when a model is too simple to capture the underlying patterns in the training data, leading to poor performance on both training and test data.

#### Univariate analysis
Univariate analysis examines a single variable to summarize its main characteristics using descriptive statistics and visualizations.

#### Unstructured data
Unstructured data lacks a predefined format or organization (e.g., text, images), requiring specialized techniques for analysis.

#### Unsupervised learning
Unsupervised learning is a machine learning approach that analyzes unlabeled data to find hidden patterns or intrinsic structures.

</details>


<details>

<summary>V</summary>

#### Variance
Variance is a statistical measure that quantifies the spread of data points around the mean.

#### Violin plot
A violin plot combines a box plot and a kernel density plot to show the distribution of a numerical variable, including its density and summary statistics.

</details>


<details>

<summary>W</summary>

#### 

</details>


<details>

<summary>X</summary>

#### XGBoost
XGBoost (Extreme Gradient Boosting) is a popular and efficient implementation of gradient boosting used for classification and regression tasks.

</details>


<details>

<summary>Y</summary>

#### 

</details>


<details>

<summary>Z</summary>

#### *Z*-score
A *Z*-score indicates how many standard deviations a data point is from the mean, and is used to standardize data and detect outliers.

#### *Z*-test
A *Z*-test is a statistical test used to determine whether the population mean and the sample mean differ significantly, applied when the population variance is known and the sample size is large.

</details>
